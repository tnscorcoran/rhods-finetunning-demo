{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4416e93",
   "metadata": {},
   "source": [
    "# Here we pull a vLLM compatible LLM (mistral-7b-instruct-v0.2) from Huggingface\n",
    "\n",
    "Some useful links:\n",
    " - Create a vLLM Serving Runtime in RHOAI: https://github.com/rh-aiservices-bu/llm-on-openshift/blob/main/serving-runtimes/vllm_runtime/README.md\n",
    " - Supported Models in vLLM: https://docs.vllm.ai/en/latest/models/supported_models.html\n",
    " - yaml to create a vLLM Single Model Server in RHOAI: https://github.com/rh-aiservices-bu/llm-on-openshift/blob/main/serving-runtimes/vllm_runtime/vllm-runtime.yaml - download it and add these args on lines 20 and 21\n",
    "        - --max-model-len\n",
    "        - \"8192\"\n",
    "\n",
    " - How to make API calls to vLLM servers: https://docs.vllm.ai/en/latest/getting_started/quickstart.html#using-openai-completions-api-with-vllm\n",
    " - LLMs in RHOAI: https://ai-on-openshift.io/demos/llm-chat-doc/llm-chat-doc/#llm-serving-solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a783237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/git-lfs/git-lfs/releases/download/v3.4.0/git-lfs-linux-amd64-v3.4.0.tar.gz\n",
    "!tar -xvzf git-lfs-linux-amd64-v3.4.0.tar.gz\n",
    "!PREFIX=/opt/app-root/src/.local ./git-lfs-3.4.0/install.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553340c-1d04-4428-8f98-10b9f84f526b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN=\"YOUR_HUGGINGFACE_TOKEN\"\n",
    "!pip install --upgrade huggingface_hub\n",
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a2c5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d55e0a-71d0-4150-87d9-e2381a59d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to use the model we fine tuned in llamafinetune_demo.ipynb and pushed to Huggingface\n",
    "#!git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "\n",
    "# Let's default to Llama2 7B\n",
    "!git clone https://huggingface.co/Trelis/Llama-2-7b-chat-hf-sharded-bf16\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
